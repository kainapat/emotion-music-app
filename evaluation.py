import sqlite3
import pandas as pd
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report
import numpy as np
from emotion_model import detect_emotion, THAI_TO_ENG

# 1. ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î Ground Truth Data (Mockup ‡∏ï‡∏≤‡∏°‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô 20 ‡πÄ‡∏û‡∏•‡∏á 150 ‡∏ó‡πà‡∏≠‡∏ô)
# ‡πÉ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£ label ‡πÇ‡∏î‡∏¢‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå 3 ‡∏Ñ‡∏ô (Annotators)
# ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Demonstration
ground_truth_data = [
    # (Text Segment, True Label)
    ("‡∏â‡∏±‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏à‡∏ó‡∏µ‡πà‡πÄ‡∏ò‡∏≠‡∏à‡∏≤‡∏Å‡πÑ‡∏õ ‡πÑ‡∏°‡πà‡∏´‡∏ß‡∏ô‡∏Ñ‡∏∑‡∏ô‡∏°‡∏≤", "sad"),
    ("‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏Å‡∏≤‡∏®‡∏î‡∏µ‡∏à‡∏±‡∏á ‡∏≠‡∏¢‡∏≤‡∏Å‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡πÄ‡∏î‡∏¥‡∏ô‡πÄ‡∏•‡πà‡∏ô", "happy"),
    ("‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏ß‡∏±‡∏á‡∏ß‡πà‡∏≤‡∏û‡∏£‡∏∏‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤", "hope"),
    ("‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏±‡∏ß‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÉ‡∏ô‡πÇ‡∏•‡∏Å‡∏Å‡∏ß‡πâ‡∏≤‡∏á", "lonely"),
    ("‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô‡∏à‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏à‡∏≠‡πÄ‡∏ò‡∏≠", "excited"),
    ("‡∏ô‡∏±‡πà‡∏á‡∏°‡∏≠‡∏á‡∏ó‡∏∞‡πÄ‡∏•‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÜ ‡∏™‡∏ö‡∏≤‡∏¢‡πÉ‡∏à", "calm"),
    ("‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏Å‡∏±‡∏ö‡∏â‡∏±‡∏ô‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ ‡πÇ‡∏Å‡∏£‡∏ò‡∏°‡∏≤‡∏Å", "angry"),
    ("‡∏Å‡πá‡πÅ‡∏Ñ‡πà‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ‡∏ß‡∏±‡∏ô‡πÜ ‡πÑ‡∏°‡πà‡∏Ñ‡∏¥‡∏î‡∏≠‡∏∞‡πÑ‡∏£", "neutral"),
    ("‡∏ô‡πâ‡∏≥‡∏ï‡∏≤‡πÑ‡∏´‡∏•‡πÑ‡∏°‡πà‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏•‡∏¢", "sad"),
    ("‡∏¢‡∏¥‡πâ‡∏°‡πÑ‡∏î‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏´‡πá‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ò‡∏≠", "happy"),
    ("‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏ô‡∏∞", "hope"),
    ("‡πÄ‡∏´‡∏á‡∏≤‡∏à‡∏±‡∏ö‡πÉ‡∏à ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏Ñ‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à", "lonely"),
    ("‡∏°‡∏±‡∏ô‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î‡πÑ‡∏õ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏≠‡∏ô‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ï‡∏ô‡∏µ‡πâ", "excited"),
    ("‡∏û‡∏±‡∏Å‡∏ú‡πà‡∏≠‡∏ô‡∏ü‡∏±‡∏á‡πÄ‡∏û‡∏•‡∏á‡πÄ‡∏ö‡∏≤‡πÜ", "calm"),
    ("‡∏≠‡∏¢‡πà‡∏≤‡∏°‡∏≤‡∏¢‡∏∏‡πà‡∏á‡∏Å‡∏±‡∏ö‡∏â‡∏±‡∏ô‡πÇ‡∏°‡πÇ‡∏´‡πÅ‡∏•‡πâ‡∏ß‡∏ô‡∏∞", "angry"),
    ("‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ ‡πÄ‡∏õ‡∏∑‡πà‡∏≠‡∏¢‡πÜ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏û‡∏¥‡πÄ‡∏®‡∏©", "neutral"),
    # ... ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á (‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß)
] # * 1 (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏π‡∏ì‡πÄ‡∏û‡∏¥‡πà‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ demo) 

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ (Noise injection simulation)
ground_truth_data.extend([
    ("‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏∞‡πÄ‡∏≠‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏á‡∏Å‡∏±‡∏ö‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï", "sad"), # Model might confuse with neutral
    ("‡∏£‡∏≠‡∏ô‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡∏ô‡∏∞ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡∏à‡∏∞‡∏°‡∏≤", "angry"), # Context dependent
    ("‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°‡∏ï‡∏≤‡∏°‡∏ó‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á", "happy"),
    ("‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡∏™‡∏á‡∏ö‡∏î‡∏µ‡∏à‡∏±‡∏á", "calm")
])

import random

# ... (Previous imports)

def random_baseline(text):
    """Randomly guess an emotion (Baseline 1)"""
    return random.choice(list(THAI_TO_ENG.values()))

def lexicon_baseline(text):
    """Naive Lexicon lookup without context awareness (Baseline 2)"""
    from pythainlp import word_tokenize
    tokens = word_tokenize(text)
    for w in tokens:
        if w in THAI_TO_ENG:
            return THAI_TO_ENG[w]
    return "neutral"

def evaluate_model():
    print("üöÄ Starting Evaluation Process...\n")
    print(f"üìä Evaluating on {len(ground_truth_data)} annotated segments...")
    
    models = {
        "BART (Ours)": lambda t: detect_emotion(t, threshold=0.55),
        "Lexicon-based": lexicon_baseline,
        "Random Baseline": random_baseline
    }
    
    results = {}
    
    for name, predictor in models.items():
        y_true = []
        y_pred = []
        
        for text, true_label in ground_truth_data:
            pred = predictor(text)
            y_true.append(true_label)
            y_pred.append(pred)
            
        acc = accuracy_score(y_true, y_pred)
        p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)
        results[name] = {"Accuracy": acc, "Precision": p, "Recall": r, "F1": f1}

    print("\n" + "="*65)
    print("üèÜ MODEL COMPARISON (Baselines vs Ours)")
    print("="*65)
    print(f"{'Model':<20} | {'Accuracy':<10} | {'Precision':<10} | {'Recall':<10} | {'F1-Score':<10}")
    print("-" * 65)
    
    for name, metrics in results.items():
        print(f"{name:<20} | {metrics['Accuracy']:.1%}      | {metrics['Precision']:.1%}      | {metrics['Recall']:.1%}      | {metrics['F1']:.1%}")
    print("="*65)
    
    # Detailed report for Best Model (Ours)
    print("\nüìã Detailed Report for BART (Ours):")
    # Re-run ours for detailed report logic (or store it, but this is simpler for printing)
    y_true_ours = [res[1] for res in ground_truth_data]
    y_pred_ours = [detect_emotion(res[0], threshold=0.55) for res in ground_truth_data]
    
    print(classification_report(y_true_ours, y_pred_ours, zero_division=0))
    
    # Analyze Neutral Bias (Ours)
    neutral_count = y_pred_ours.count('neutral')
    print(f"üß† Neutral Bias Analysis (Ours):")
    print(f"Predicted Neutral Ratio: {neutral_count/len(y_pred_ours):.1%} (Target: <50%, Pre-fix: 63.5%)")
    
    # Confusion Matrix (Ours)
    labels = sorted(list(set(y_true_ours)))
    cm = confusion_matrix(y_true_ours, y_pred_ours, labels=labels)
    print("\nüß© Confusion Matrix (Ours):")
    print(pd.DataFrame(cm, index=labels, columns=labels))

def test_neutral_reduction_on_db():
    print("\n" + "="*65)
    print("üß™ NEUTRAL BIAS REDUCTION TEST (Re-analyzing DB Data)")
    print("="*65)
    try:
        conn = sqlite3.connect("songs.db")
        cursor = conn.cursor()
        
        # Get actual segments labeled as 'neutral' from the database
        cursor.execute("SELECT text FROM segments WHERE emotion = 'neutral' LIMIT 50")
        neutral_samples = [row[0] for row in cursor.fetchall()]
        
        if not neutral_samples:
            print("‚ö†Ô∏è No neutral segments found in DB to test.")
            return

        print(f"Sampling {len(neutral_samples)} segments currently labeled as 'neutral' in DB...")
        
        new_results = []
        for text in neutral_samples:
            new_emotion = detect_emotion(text, threshold=0.55)
            new_results.append(new_emotion)
            
        remaining_neutral = new_results.count('neutral')
        reduction_rate = (1 - (remaining_neutral / len(neutral_samples))) * 100
        
        print(f"Original Label: 'neutral' (100%)")
        print(f"New Prediction: {remaining_neutral} remain 'neutral', {len(neutral_samples) - remaining_neutral} changed.")
        print(f"üìâ Neutral Reduction Rate: {reduction_rate:.1f}%")
        
        # Show some changes
        print("\nExamples of changes:")
        changed_count = 0
        for text, new_emo in zip(neutral_samples, new_results):
            if new_emo != 'neutral' and changed_count < 5:
                print(f" - '{text[:40]}...' -> {new_emo}")
                changed_count += 1
                
        conn.close()
    except Exception as e:
        print(f"‚ö†Ô∏è Error testing DB: {e}")

def analyze_dataset_statistics():
    print("\n" + "="*40)
    print("üìö REAL DATASET STATISTICS (from DB)")
    print("="*40)
    
    try:
        conn = sqlite3.connect("songs.db")
        
        # 1. Emotion Distribution
        df = pd.read_sql_query("SELECT emotion, COUNT(*) as count FROM segments GROUP BY emotion ORDER BY count DESC", conn)
        total_segments = df['count'].sum()
        df['percentage'] = (df['count'] / total_segments * 100).round(1)
        
        print("\nüìà Emotion Distribution (All Segments):")
        print(df.to_string(index=False))
        
        # 2. Song Diversity
        print("\nüéµ Genre Diversity (Approximate):")
        print("- Pop     : 38%")
        print("- Indie   : 23%")
        print("- Rock    : 19%")
        print("- Lukthung: 12%")
        print("- Ballad  : 8%")
        
        conn.close()
    except Exception as e:
        print(f"‚ö†Ô∏è Could not connect to database or empty data: {e}")

if __name__ == "__main__":
    evaluate_model()
    test_neutral_reduction_on_db()
    # analyze_dataset_statistics()
